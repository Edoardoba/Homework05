{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import statistics\n",
    "import pickle\n",
    "from collections import defaultdict \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build the graph\n",
    "\n",
    "We had to build the graph properly and provide basic informations such as:\n",
    "\n",
    "   * If it is direct or not\n",
    "   * The number of nodes\n",
    "   * The number of edges\n",
    "   * The average node degree. Is the graph dense?\n",
    "   \n",
    "\n",
    "We used a package called **networkX** that has been used in the labs with prof Ioannis.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"wiki-topcats-reduced.txt\", create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the built in function of networkX to get all these basics informations. We have **461193** nodes and an impressive **2645247** number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: DiGraph\\nNumber of nodes: 461193\\nNumber of edges: 2645247\\nAverage in degree:   5.7357\\nAverage out degree:   5.7357'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph that we deal with is directed graph according to the *\"wiki-topcats-reduced.txt\"*. In this text file, every row is an edge, the two elements are the nodes (source and destination). Some pairs of nodes are shown twice while the source and destination are opposite. Thus,  we decide that this graph is a **directed graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this graph is directed , the **average degree** is the number of edges divided by the number nodes: \n",
    "\n",
    "$$Average \\ degree = \\frac{the \\ number \\ of \\ edges}{the \\ number \\ of \\ nodes}$$\n",
    "\n",
    "To retrieve this information we just had to use the followinf networkX functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.735661642739591"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges() / G.number_of_nodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last information we had to provide about this graph is its **density**. The density of a directed graph is defined as:\n",
    "\n",
    "\n",
    "$$Density = \\frac{the \\ number \\ of \\ edges}{the \\ number \\ of \\ nodes(the \\ number \\ of \\ nodes - 1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2436602635647606e-05\n",
      "1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "density = G.number_of_edges() / (G.number_of_nodes()* (G.number_of_nodes()- 1))\n",
    "print(density)\n",
    "print(nx.density(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly say that this graph is **not dense**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: DiGraph\\nNumber of nodes: 461193\\nNumber of edges: 2645247\\nAverage in degree:   5.7357\\nAverage out degree:   5.7357'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called again the info function just to double-check our results. The **average degree**, number of nodes and edges are the same as the ones we obtained before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# RQ2\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a _block-ranking_, where the blocks are represented by the categories\n",
    "   \n",
    "   - How to order categories?\n",
    "       1. Pick first category $C_0$\n",
    "       2. compute distance between nodes in the category($C_0$) and nodes of other categories($C_i$)\n",
    "       \n",
    "       $$distance(C_0, C_i)  = median(ShortestPath(C_0, C_i))$$\n",
    "              \n",
    "           - $ShortestPath(C_0, C_i)$ is the set of all the possible shortest paths between the nodes of $C_0$ and $C_i$\n",
    "           - length of a path is given by the sum of the weights of the edges\n",
    "           \n",
    "### Once you obtain the _block-ranking_ vector, we sort the nodes in each category.\n",
    "    \n",
    "   - How to sort the nodes?\n",
    "       1. Compute subgraph induced by $C_0$. For each node compute the sum of the weigths of the in‑edges.\n",
    "          $$score_{article_i} = \\sum_{i\\in{in-edges}} w_i$$ \n",
    "          \n",
    "       2. Extend the graph to the nodes that belong to $C_1$. Thus, for each article in $C_1$ compute the score as before. \n",
    "          **Note** that the in‑edges coming from the previous category,$C_0$, have as weights the score of the node that sends the edge.\n",
    "    \n",
    "       3. Repeat Step2 up to the last category of the ranking. \n",
    "       \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing the files we need. We first import *wiki-topcats-reduced.txt* that contains nodes(source and destinations) and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the numbe of nodes in this file\n",
    "all_nodes = []\n",
    "with open(\"wiki-topcats-reduced.txt\") as file:\n",
    "    for edge in file:\n",
    "        all_nodes.extend(edge.split())\n",
    "all_nodes = set(all_nodes)\n",
    "\n",
    "# the number of nodes\n",
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = []\n",
    "with open(\"wiki-topcats-reduced.txt\") as file:\n",
    "    for edge in file:\n",
    "        edges.append(edge.split())\n",
    "        \n",
    "# the number of edges\n",
    "len(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then computed,to explore properly the data-set, the *distinct* number of sources and destinations in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428957"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want to know how many different sources we have in the above file\n",
    "sources = []\n",
    "for nodes in edges:\n",
    "    sources.append(nodes[0])\n",
    "sources = set(sources)\n",
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want to know how many different destinations we have in the above file\n",
    "destinations = []\n",
    "for nodes in edges:\n",
    "    destinations.append(nodes[1])\n",
    "destinations = set(destinations)\n",
    "destinations = sorted(destinations, key=int)\n",
    "len(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceeded importing the *wiki-topcats-categories.txt* file that contains all the different categories and all the articles belonging to the. We consider only categories having more than **3500 articles**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categories dictionary\n",
    "categories0 = {}\n",
    "with open(\"wiki-topcats-categories.txt\") as file:\n",
    "    for category in file:\n",
    "        category = category.strip(\"Category:\").replace(\"\\n\", \"\").replace(\";\", \"\").split()\n",
    "        categories0[category[0]] = category[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract only categories which have 3500 articles\n",
    "categories = {}\n",
    "for k,v in categories0.items():\n",
    "    if len(v) > 3500:\n",
    "        categories[k] = v\n",
    "        \n",
    "# the number of categories that we deal with\n",
    "len(categories.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to consider only articles present in the reduced file. We took the intersection in order to cut out the nodes of which we had no information about. We print out the result to show the numerosity of every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English_footballers': 7538,\n",
       " 'The_Football_League_players': 7814,\n",
       " 'Association_football_forwards': 5097,\n",
       " 'Association_football_goalkeepers': 3737,\n",
       " 'Association_football_midfielders': 5827,\n",
       " 'Association_football_defenders': 4588,\n",
       " 'Living_people': 348300,\n",
       " 'Year_of_birth_unknown': 2536,\n",
       " 'Harvard_University_alumni': 5549,\n",
       " 'Major_League_Baseball_pitchers': 5192,\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies': 6491,\n",
       " 'Indian_films': 5568,\n",
       " 'Year_of_death_missing': 4122,\n",
       " 'English_cricketers': 3275,\n",
       " 'Year_of_birth_missing_(living_people)': 28498,\n",
       " 'Rivers_of_Romania': 7729,\n",
       " 'Main_Belt_asteroids': 11660,\n",
       " 'Asteroids_named_for_people': 4895,\n",
       " 'English-language_albums': 4760,\n",
       " 'English_television_actors': 3362,\n",
       " 'British_films': 4422,\n",
       " 'English-language_films': 22463,\n",
       " 'American_films': 15159,\n",
       " 'Fellows_of_the_Royal_Society': 3446,\n",
       " 'People_from_New_York_City': 4614,\n",
       " 'American_Jews': 3411,\n",
       " 'American_television_actors': 11531,\n",
       " 'American_film_actors': 13865,\n",
       " 'Debut_albums': 7561,\n",
       " 'Black-and-white_films': 10759,\n",
       " 'Year_of_birth_missing': 4346,\n",
       " 'Place_of_birth_missing_(living_people)': 5532,\n",
       " 'Article_Feedback_Pilot': 3485,\n",
       " 'American_military_personnel_of_World_War_II': 3720,\n",
       " 'Windows_games': 4025}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cate,nodes in categories.items():\n",
    "    \n",
    "    # Get only intersection between nodes in category and all nodes\n",
    "    categories[cate] = list(set(nodes).intersection(all_nodes))\n",
    "\n",
    "num_nodes_reduced = {}\n",
    "for cate,nodes in categories.items():\n",
    "    num_nodes_reduced[cate] = len(nodes)\n",
    "\n",
    "# this is the categories and the number of nodes that we will work with\n",
    "num_nodes_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the homework text, we had to possibility to choose the category in input. We opted to choose the less populated one,**'Year_of_birth_unknown'**, in order to make our computations less heavier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = 'Year_of_birth_unknown'\n",
    "all_ci = list(categories.keys())\n",
    "all_ci.remove(c0) #we remove c0 form the list of all the categories in which we have to compute the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_c0 = categories[c0] #this are the nodes corresponding to c0 category\n",
    "nodes_c0.sort(key=int)\n",
    "len(nodes_c0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined two functions in order to be able to use picke files. We used that to save the ranked result(our output) in an external file in order to be able to load it up when needed without making all the computations again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_to_pickle(file, content):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(content, f)\n",
    "        f.close()\n",
    "\n",
    "def read_file_from_pickle(file):\n",
    "    file_content = {}\n",
    "    with open(file, \"rb\") as f:\n",
    "        file_content = pickle.load(f)\n",
    "        f.close()\n",
    "    \n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore our graph, we decided to use Breadth-first search (BFS). It is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph) and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level. \n",
    "\n",
    "This algorithm has mainly two features(the reasons why we opted to choose this one):\n",
    "\n",
    "*     it uses a queue (First In First Out) instead of a stack (Last In First Out) and\n",
    "*     it checks whether a vertex has been discovered before enqueueing the vertex rather than delaying this check until the vertex is dequeued from the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(Graph, source, destination):\n",
    "    queue = [[source]]  # we take the set of the sources\n",
    "    visited = set()  #we create a set in which we will store all the visited nodes\n",
    "    \n",
    "    if not nx.has_path(Graph, source, destination): #If there is no path between source and destination we return nan\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    while queue:   #While we have some nodes to explore\n",
    "        # Gets the first path in the queue\n",
    "        path = queue.pop(0)\n",
    "\n",
    "        # Gets the last node in the path\n",
    "        vertex = path[-1]\n",
    "\n",
    "        # Checks if we got to the end\n",
    "        if vertex == destination:\n",
    "            return len(path)\n",
    "        # We check if the current node is already in the visited nodes set in order not to recheck it\n",
    "        elif vertex not in visited:\n",
    "            neighbours = list(Graph.neighbors(vertex))\n",
    "            # enumerate all adjacent nodes, construct a new path and push it into the queue\n",
    "            for current_neighbour in neighbours:\n",
    "                new_path = list(path)\n",
    "                new_path.append(current_neighbour)\n",
    "                queue.append(new_path)\n",
    "                \n",
    "            # Mark the vertex as visited\n",
    "            visited.add(vertex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ranking = {} #This will contain the categories ranked.\n",
    "\n",
    "for ci in all_ci:\n",
    "    # pick one category\n",
    "    nodes_ci = categories[ci]\n",
    "    \n",
    "    #make a subgraph\n",
    "    sub_nodes = nodes_c0 + nodes_ci\n",
    "    H = G.subgraph(sub_nodes)\n",
    "    \n",
    "    # a list to input the shortest path\n",
    "    length = []\n",
    "\n",
    "    for source in nodes_c0:\n",
    "        for destination in nodes_ci:\n",
    "            # compute shortest path\n",
    "            length.append(bfs(H, source, destination))\n",
    "    \n",
    "    # remove nan value\n",
    "    length = [ x for x in length if str(x) != \"nan\"]\n",
    "    # get a median in list and append it to the dictionary\n",
    "    block_ranking[ci] = statistics.median(length)\n",
    "    \n",
    "# sort by value\n",
    "block_ranking = sorted(block_ranking.items(), key = lambda x: x[1])\n",
    "\n",
    "# save this dictionary to avoid costing much time again \n",
    "write_file_to_pickle(\"block_ranking\", block_ranking)\n",
    "block_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having read from the pickle file this is the output of our categories ranked by the median of the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Major_League_Baseball_pitchers', 2),\n",
       " ('Debut_albums', 2),\n",
       " ('Year_of_death_missing', 3.0),\n",
       " ('Year_of_birth_missing_(living_people)', 3),\n",
       " ('Year_of_birth_missing', 3),\n",
       " ('Place_of_birth_missing_(living_people)', 3),\n",
       " ('Asteroids_named_for_people', 4.5),\n",
       " ('Main_Belt_asteroids', 5),\n",
       " ('British_films', 5),\n",
       " ('Association_football_goalkeepers', 7),\n",
       " ('Article_Feedback_Pilot', 7),\n",
       " ('Rivers_of_Romania', 7.0),\n",
       " ('English_footballers', 8),\n",
       " ('The_Football_League_players', 8.0),\n",
       " ('Association_football_forwards', 8.0),\n",
       " ('Association_football_midfielders', 8.0),\n",
       " ('Association_football_defenders', 8.0),\n",
       " ('English_cricketers', 8.0),\n",
       " ('English_television_actors', 8.0),\n",
       " ('Fellows_of_the_Royal_Society', 8.0),\n",
       " ('American_Jews', 8),\n",
       " ('American_television_actors', 8),\n",
       " ('American_film_actors', 8.0),\n",
       " ('American_military_personnel_of_World_War_II', 8),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 8),\n",
       " ('Living_people', 8.0),\n",
       " ('Indian_films', 9),\n",
       " ('English-language_albums', 9),\n",
       " ('Harvard_University_alumni', 10.0),\n",
       " ('People_from_New_York_City', 11),\n",
       " ('English-language_films', 12.0),\n",
       " ('American_films', 12.0),\n",
       " ('Black-and-white_films', 14.0),\n",
       " ('Windows_games', 24.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking = read_file_from_pickle(\"block_ranking\")\n",
    "block_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE NEED JUST THIS PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_block_ranking = [x[0] for x in block_ranking ]\n",
    "categories_block_ranking.insert(0, c0)\n",
    "len(categories_block_ranking)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make nested dictionary(key1 : category, key2(value1) : node, value2 :  weight)\n",
    "nodes_rank = {}\n",
    "\n",
    "for idx, category in enumerate(categories_block_ranking):\n",
    "    \n",
    "    # nested dictionary like \n",
    "    # nodes_rank { category { node : weight } }\n",
    "    \n",
    "    nodes_rank[category] = {}\n",
    "    nodes = categories[category]\n",
    "    \n",
    "    # This is only first category\n",
    "    if idx == 0:\n",
    "        \n",
    "        # make subgraph\n",
    "        H = G.subgraph(nodes)\n",
    "        \n",
    "        for node in nodes:\n",
    "            # first set 0 as initial weight\n",
    "            nodes_rank[category][node]  = 0\n",
    "            # find neighbors\n",
    "            neighbors = list(H.neighbors(node))\n",
    "            \n",
    "            for neighbor in neighbors:\n",
    "                # assign weight \n",
    "                # check whether the edge is coming from neighbor to node or not\n",
    "                if nx.has_path(H, neighbor, node):\n",
    "                    nodes_rank[category][node] += 1\n",
    "        \n",
    "    else:\n",
    "        # make subgraph of current category and previous category\n",
    "        sub_nodes = nodes + previous_nodes\n",
    "        H = G.subgraph(sub_nodes)\n",
    "        \n",
    "        for node in nodes:\n",
    "            # first set 0 as initial weight\n",
    "            nodes_rank[category][node]  = 0\n",
    "            # find neighbors\n",
    "            neighbors = list(H.neighbors(node))\n",
    "            \n",
    "            for neighbor in neighbors:\n",
    "                # if the edge comes from PREVIOUS category, add the weight of the node which comes from\n",
    "                if neighbor in previous_nodes and nx.has_path(H, neighbor, node):\n",
    "                    nodes_rank[category][node] += nodes_rank[previous_category][neighbor]\n",
    "                    continue\n",
    "                # if the edge comes from CURRENT category\n",
    "                elif nx.has_path(H, neighbor, node):\n",
    "                    nodes_rank[category][node] += 1\n",
    "                    \n",
    "    # save nodes and category as previous data         \n",
    "    previous_nodes = nodes\n",
    "    previous_category = category\n",
    "\n",
    "# sort values in dictionary\n",
    "for i in range(len(nodes_rank)):\n",
    "    category = categories_block_ranking[i]\n",
    "    nodes_rank[category] = sorted(nodes_rank[category].items(), key = lambda x: x[1], reverse=True)    \n",
    "\n",
    "# save as a pickle file\n",
    "write_file_to_pickle(\"sorted_nodes\", nodes_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Year_of_birth_unknown', 'Major_League_Baseball_pitchers', 'Debut_albums', 'Year_of_death_missing', 'Year_of_birth_missing_(living_people)', 'Year_of_birth_missing', 'Place_of_birth_missing_(living_people)', 'Asteroids_named_for_people', 'Main_Belt_asteroids', 'British_films', 'Association_football_goalkeepers', 'Article_Feedback_Pilot', 'Rivers_of_Romania', 'English_footballers', 'The_Football_League_players', 'Association_football_forwards', 'Association_football_midfielders', 'Association_football_defenders', 'English_cricketers', 'English_television_actors', 'Fellows_of_the_Royal_Society', 'American_Jews', 'American_television_actors', 'American_film_actors', 'American_military_personnel_of_World_War_II', 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 'Living_people', 'Indian_films', 'English-language_albums', 'Harvard_University_alumni', 'People_from_New_York_City', 'English-language_films', 'American_films', 'Black-and-white_films', 'Windows_games'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_rank = read_file_from_pickle(\"sorted_nodes\")\n",
    "nodes_rank.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1656777', 6), ('1656793', 5), ('34422', 4), ('170969', 4), ('170970', 4)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_rank[\"Year_of_birth_unknown\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
